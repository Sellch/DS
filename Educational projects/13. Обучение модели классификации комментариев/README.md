# Обучение модели классификации комментариев

## Данные

Данные находятся в файле `toxic_comments.csv`. \
Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак.

## Задача

Определение токсичности комментариев.

## Используемые библиотеки
*Pandas*, *NumPy*, *matplotlib.pyplot*,  *Scikit-Learn*, *os.path*, *nltk.corpus*, *re*, *tqdm*, *keras.preprocessing.sequence*, *transformers*, *torch*, *Bert*

## Итоги исследования
<div style="border:solid orange 2px; padding: 5px">

<div class="alert alert-info"> <b>При проведении исследования (на основе полученных данных) выполнено:</b></div>

- Данные разделены на 3 части: для обучения моделей, для валидации и данные для проведения финального тестирования в соотношении 70 - 10 - 20 соответственно.
- Проведено токенезация корпусов для тренировочной и тестовой выборки.
- Созданы attention-mask для тренировочной и тестовой выборки.
- Данные преобразованы в векторной представление `input_ids`.
- Произведено дообучение Bert модели с использованием GPU.
- Произведена оценка модели на валидационной выборке.
- Получен показатель F1-score на тестовой выборке равный 0.84.